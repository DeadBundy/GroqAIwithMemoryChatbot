# ğŸ’¬ Baby AI â€” Groq-Powered Conversational Web App

### ğŸš€ Overview
**Baby AI** is a lightweight conversational AI web application powered by **Groq LLMs** and built using **FastAPI** and **vanilla JavaScript**.  
It demonstrates full-stack AI integration â€” from backend API design to frontend interactivity â€” showcasing real-world skills for AI Developer or Copilot Engineer roles.

---

### ğŸ§  Whatâ€™s Inside
- ğŸ’¬ **Conversational Chat** â€” context-aware responses powered by Groqâ€™s `llama-3.3-70b-versatile`
- ğŸ§¾ **Summarization Tool** â€” generate short, bullet, or detailed summaries
- âœï¸ **Rewrite Assistant** â€” rewrite any text in various tones (formal, casual, friendly)
- ğŸ§© **Memory & Reset** â€” maintains session-based chat memory
- ğŸ¨ **Web Interface** â€” built with pure HTML, CSS, and vanilla JS (no frameworks)

---

### âš™ï¸ Tech Stack
| Layer | Technology |
|--------|-------------|
| **Language Model** | [Groq API](https://groq.com/) â€” high-speed inference for open LLMs |
| **Backend** | Python 3.13 + FastAPI |
| **Frontend** | HTML + CSS + JavaScript (Vanilla JS) |
| **Data Handling** | Pydantic models + in-memory session storage |
| **Version Control** | Git + GitHub |

---

### ğŸ§© What is an LLM?
A **Large Language Model (LLM)** is an AI model trained on massive text datasets to understand and generate human-like language.  
LLMs are the core of AI copilots â€” they read, reason, and reply intelligently.  
Baby AI uses **Groq-hosted LLMs**, offering ultra-low latency, real-time responses, and fast inference for production-scale AI apps.

---

### ğŸŒŸ Screenshots

#### ğŸ’¬ Chat Interface
![Chat UI](screenshots/chatbotUI.PNG)
![Chat UI](screenshots/chatbot.png)

#### ğŸ§  Summarization & Rewrite Tools
![Tools](screenshots/chatbot_subtasks.png)
![Chat UI](screenshots/chatbot_summarize.png)
![Chat UI](screenshots/chatbotrewrite.png)

#### âš™ï¸ FastAPI Interactive Docs
![Docs](screenshots/screenshot_docs.png)

> Powered by Groq LLMs, built from scratch with FastAPI + Vanilla JavaScript.

---


### ğŸ§° How to Run Locally

# clone the repo
git clone https://github.com/DeadBundy/baby-ai.git
cd baby-ai

# create and activate a virtual environment
python -m venv .venv
.\.venv\Scripts\activate  # on Windows

# install dependencies
pip install -r requirements.txt

# run the backend
python -m uvicorn server:app --reload --host 127.0.0.1 --port 8001

#Then open your browser:
ğŸ‘‰ http://127.0.0.1:8001/chat-ui

This project demonstrates:
âš™ï¸ Integration of real-world LLM APIs (Groq)
ğŸ§© Backend design with FastAPI + Pydantic
ğŸŒ Frontend built with vanilla JavaScript (no libraries)
ğŸ§  Context memory and session handling
ğŸ’¬ Smooth human-like interaction loop

It mirrors how modern AI copilots work:
â†’ take user input â†’ process with LLM â†’ return a refined, contextual reply.

ğŸ§‘â€ğŸ’» Author
Meghana S
AI & Software Enthusiast | Building Intelligent Systems with Purpose 
Built with ğŸ’œ using Python, Groq, and FastAPI.
